{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11541,
     "status": "ok",
     "timestamp": 1582541684670,
     "user": {
      "displayName": "Виталий Гречачин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDZJF3eghaET6SaBIhXkw2Ij3k2tPFd5G2ZbRvU8g=s64",
      "userId": "00685398523661527923"
     },
     "user_tz": -300
    },
    "id": "4oYPfzQB3bBs",
    "outputId": "df0cdf3d-3828-4fc1-d4ec-dca099c80ff3"
   },
   "outputs": [],
   "source": [
    "# Импортируем необходимые инструменты\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ML\n",
    "import sklearn\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "# Graphs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# My preprocessing tools\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19230,
     "status": "ok",
     "timestamp": 1582541744073,
     "user": {
      "displayName": "Виталий Гречачин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDZJF3eghaET6SaBIhXkw2Ij3k2tPFd5G2ZbRvU8g=s64",
      "userId": "00685398523661527923"
     },
     "user_tz": -300
    },
    "id": "IA1yKYjS89ZV",
    "outputId": "33513f12-44d0-40b7-8fd1-e77e940aa97f"
   },
   "outputs": [],
   "source": [
    "# Настроим расположение файлов для работы в Google Colab или на своем рабочем месте\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False\n",
    "\n",
    "if is_in_colab:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    drive.mount('/content/drive')\n",
    "    data_folder = r'/content/drive/My Drive/Colab/toxic/'\n",
    "else:\n",
    "    data_folder = r'./data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLWvLq9G9RUS"
   },
   "outputs": [],
   "source": [
    "# Загружаем необходимые данные: датасет, стоп слова, ненормативная лексика.\n",
    "# Я заранее подготовил текстовый файл \"badwords.txt\" со списком ненормативной лексики.\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "with open(data_folder + 'badwords.txt') as file:  \n",
    "  bad_words = set(file.read().split('\\n'))\n",
    "data = pd.read_csv(data_folder + '/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9m_HqCH10lI"
   },
   "source": [
    "## Посмотрим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1582541892143,
     "user": {
      "displayName": "Виталий Гречачин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDZJF3eghaET6SaBIhXkw2Ij3k2tPFd5G2ZbRvU8g=s64",
      "userId": "00685398523661527923"
     },
     "user_tz": -300
    },
    "id": "5ASl1HVw2A8I",
    "outputId": "44ac3f46-80d7-4b82-e93c-fae1c48c69f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barcelona football seasons (early 20th century...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\n answering questions \\n\\nI won't have a c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\nIssues\\nOnefortyone, the reason why I rem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nIn that case would \"\"new age\"\" at least be ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Wolf,\\nWe need to have a serious talk abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Please note that DPeterson has now starting a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bald Eagle\\nI've addressed all your points so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SARAH \\n\\nYou deleted it, but it remains. I em...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rand Paul \\n\\nAre you monitoring Rand Paul?  I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Poor AFD rationales \\n Wikipedia:Articles for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Barcelona football seasons (early 20th century...      0\n",
       "1  \"\\n\\n answering questions \\n\\nI won't have a c...      0\n",
       "2  \"\\n\\nIssues\\nOnefortyone, the reason why I rem...      0\n",
       "3  \"\\nIn that case would \"\"new age\"\" at least be ...      0\n",
       "4  Mr. Wolf,\\nWe need to have a serious talk abou...      0\n",
       "5  \"Please note that DPeterson has now starting a...      0\n",
       "6  Bald Eagle\\nI've addressed all your points so ...      0\n",
       "7  SARAH \\n\\nYou deleted it, but it remains. I em...      0\n",
       "8  Rand Paul \\n\\nAre you monitoring Rand Paul?  I...      0\n",
       "9  Poor AFD rationales \\n Wikipedia:Articles for ...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEPY76Cp2UjK"
   },
   "source": [
    "У нас 2 колонки: комментарий и класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1582542004978,
     "user": {
      "displayName": "Виталий Гречачин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDZJF3eghaET6SaBIhXkw2Ij3k2tPFd5G2ZbRvU8g=s64",
      "userId": "00685398523661527923"
     },
     "user_tz": -300
    },
    "id": "Oj9FNtf82hBI",
    "outputId": "8e706796-9454-4bcd-f359-97653cd08062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Узнаем количество классов\n",
    "data.toxic.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rop_wA4o2ww7"
   },
   "source": [
    "Перед нами задача бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1582542088511,
     "user": {
      "displayName": "Виталий Гречачин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDZJF3eghaET6SaBIhXkw2Ij3k2tPFd5G2ZbRvU8g=s64",
      "userId": "00685398523661527923"
     },
     "user_tz": -300
    },
    "id": "4-JDVFB-2UNi",
    "outputId": "bee4185e-ac91-48cf-ac12-b5622d854603"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09071466196190923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Узнаем соотношение классов\n",
    "data.toxic.hist()\n",
    "data.toxic.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5tKzCMU2_5v"
   },
   "source": [
    "К сожалению, мы имеем дело с несбалансированными данными. Теперь нам необходимо выбрать метрику оценки качества предсказаний. Для несбалансированных данных мы не можем использовать accuracy, поскольку если все время предсказывать нулевой класс, то мы получим accuracy 0.9. В тоже время precision и recall будут равны 0, поэтому мы можем использовать F1 для оценки качества предсказаний. Кроме F1, адекватными метриками могут послужить cohen_kappa_score и roc_auc_score. Мы будем использовать F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1582542645906,
     "user": {
      "displayName": "Виталий Гречачин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDZJF3eghaET6SaBIhXkw2Ij3k2tPFd5G2ZbRvU8g=s64",
      "userId": "00685398523661527923"
     },
     "user_tz": -300
    },
    "id": "E8R4auQx4_vq",
    "outputId": "386c404c-0072-4f21-8cb4-2f360da70e1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_text    False\n",
       "toxic           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверим nan\n",
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1582543272575,
     "user": {
      "displayName": "Виталий Гречачин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDZJF3eghaET6SaBIhXkw2Ij3k2tPFd5G2ZbRvU8g=s64",
      "userId": "00685398523661527923"
     },
     "user_tz": -300
    },
    "id": "5EWv0RD45dDm",
    "outputId": "53ecc482-d74a-4068-aeb6-803a04345a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data:\n",
      " Mean: 394.34\n",
      "Standard deviation: 589.64\n",
      "Max: 5000.00\n",
      "Min: 6.00\n",
      "\n",
      "\n",
      "Toxic comments:\n",
      " Mean: 293.02\n",
      "Standard deviation: 608.56\n",
      "Max: 5000.00\n",
      "Min: 8.00\n",
      "\n",
      "\n",
      "Non-toxic comments:\n",
      " Mean: 404.45\n",
      "Standard deviation: 586.76\n",
      "Max: 5000.00\n",
      "Min: 6.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверим на сколько длиные тексты в нашем датасете. \n",
    "# А заодно начнем искать признаки, для этого посмотрим как отличаются средние длины токсичных и нетоксичных комментариев и как отличается разброс значений.\n",
    "\n",
    "lens = data.comment_text.str.len()\n",
    "print('All data:\\n',f'Mean: {lens.mean():.2f}\\nStandard deviation: {lens.std():.2f}\\nMax: {lens.max():.2f}\\nMin: {lens.min():.2f}\\n\\n')\n",
    "lens = data.comment_text[data.toxic == 1].str.len()\n",
    "print('Toxic comments:\\n', f'Mean: {lens.mean():.2f}\\nStandard deviation: {lens.std():.2f}\\nMax: {lens.max():.2f}\\nMin: {lens.min():.2f}\\n\\n')\n",
    "lens = data.comment_text[data.toxic == 0].str.len()\n",
    "print('Non-toxic comments:\\n', f'Mean: {lens.mean():.2f}\\nStandard deviation: {lens.std():.2f}\\nMax: {lens.max():.2f}\\nMin: {lens.min():.2f}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LhUFgI88eyM"
   },
   "source": [
    "Среди всех комментариев встречаются длинные тексты. Мы видим, что токсичные комментарии короче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsdogjOi9TQl"
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "Ft8l8VsN9Vv3"
   },
   "source": [
    "Натренируем baseline модель, чтобы нам было от чего отталкиваться. В ходе экспериментов оказалось, что Logistic Regression дает лучшие результаты, нежели SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В качестве признаков, описывающих тексты, мы будем использовать TF-IDF. \n",
    "# Зачастую Count vectorizer дает хорошие результаты, но для облегчения процесса обучения, мы будем использовать именно TF-IDF, \n",
    "# поскольку полученные значения будут меньше.\n",
    "# Мы будем ипользовать в качестве признаков не только слова, но и отдельные символы. Такой подход дает лучшие результаты.\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(                                  # Vectorizer для слов\n",
    "    analyzer='word',                                                # Слова в качестве признаков\n",
    "    token_pattern=r'\\w{1,}',                                        # Шаблон для извлечения токенов\n",
    "    stop_words=stop_words,                                          # Избавимся от стоп-слов\n",
    "    ngram_range=(1, 1),                                             # Не используем n-грамы\n",
    "    max_features=20000,                                             # Максимальное количество признаков\n",
    "    max_df=0.5)                                                     # Не будем использовать токены, встречающиеся более чем в 50 процентов документов\n",
    "\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(                        # Vectorizer для символов\n",
    "    analyzer='char',                                      # Символы в качестве признаков\n",
    "    stop_words='english',                                 # Избавимся от стоп-слов\n",
    "    ngram_range=(1, 4),                                   # Используем различные n-граммы\n",
    "    max_features=30000,                                   # Максимальное количество признаков\n",
    "    max_df=0.5                                            # Не будем использовать токены, встречающиеся более чем в 50 процентов документов\n",
    "    )\n",
    "\n",
    "\n",
    "# Создадим матрицы признаков для тренировочных данных\n",
    "train_w_features = word_vectorizer.fit_transform(data.comment_text)\n",
    "train_c_features = char_vectorizer.fit_transform(data.comment_text)\n",
    "\n",
    "# Объединим все признаки в одну матрицу\n",
    "train_features = hstack([train_w_features, train_c_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score is 0.7603415794068896\n"
     ]
    }
   ],
   "source": [
    "# Наш класиификатор - Logistic Regression\n",
    "model = LogisticRegression(solver='sag')\n",
    "\n",
    "# Кросс валидация\n",
    "cv_score = np.mean(cross_val_score(model, train_features, data.toxic, cv=3, scoring='f1'))\n",
    "print('CV score is {}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также натренируем catboost, в котором есть text_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6428693\ttest: 0.6808118\tbest: 0.6808118 (0)\ttotal: 18ms\tremaining: 18s\n",
      "100:\tlearn: 0.6466302\ttest: 0.6742996\tbest: 0.6808118 (0)\ttotal: 1.67s\tremaining: 14.8s\n",
      "200:\tlearn: 0.6624235\ttest: 0.6806702\tbest: 0.6814869 (189)\ttotal: 3.3s\tremaining: 13.1s\n",
      "300:\tlearn: 0.6696244\ttest: 0.6848750\tbest: 0.6850346 (298)\ttotal: 4.91s\tremaining: 11.4s\n",
      "400:\tlearn: 0.6765890\ttest: 0.6847945\tbest: 0.6853946 (338)\ttotal: 6.52s\tremaining: 9.75s\n",
      "500:\tlearn: 0.6820485\ttest: 0.6852259\tbest: 0.6866472 (429)\ttotal: 8.14s\tremaining: 8.11s\n",
      "600:\tlearn: 0.6851456\ttest: 0.6861260\tbest: 0.6874237 (589)\ttotal: 9.81s\tremaining: 6.51s\n",
      "700:\tlearn: 0.6897311\ttest: 0.6859029\tbest: 0.6874237 (589)\ttotal: 11.5s\tremaining: 4.88s\n",
      "800:\tlearn: 0.6933936\ttest: 0.6863369\tbest: 0.6874237 (589)\ttotal: 13.1s\tremaining: 3.25s\n",
      "900:\tlearn: 0.6974754\ttest: 0.6861761\tbest: 0.6874237 (589)\ttotal: 14.7s\tremaining: 1.62s\n",
      "999:\tlearn: 0.7002507\ttest: 0.6859524\tbest: 0.6874237 (589)\ttotal: 16.4s\tremaining: 0us\n",
      "bestTest = 0.6874236874\n",
      "bestIteration = 589\n",
      "Shrink model to first 590 iterations.\n"
     ]
    }
   ],
   "source": [
    "def fit_catboost(X_train, X_test, y_train, y_test, catboost_params={}, verbose=100):\n",
    "    learn_pool = Pool(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        text_features=['comment_text'],\n",
    "        feature_names=['comment_text']\n",
    "    )\n",
    "    test_pool = Pool(\n",
    "        X_test, \n",
    "        y_test, \n",
    "        text_features=['comment_text'],\n",
    "        feature_names=['comment_text']\n",
    "    )\n",
    "    \n",
    "    catboost_default_params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'eval_metric': 'F1',\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    \n",
    "    catboost_default_params.update(catboost_params)\n",
    "    \n",
    "    model = CatBoostClassifier(**catboost_default_params)\n",
    "    model.fit(learn_pool, eval_set=test_pool, verbose=verbose)\n",
    "\n",
    "    return model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['comment_text']], data.toxic.values, test_size=0.33, random_state=42)\n",
    "cat_boost_model = fit_catboost(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили F1 = 0.687 на тестовых даных. Чтобы улучшить catboost мы можем использовать n-грамы. Но сосредоточимся на улучшении Logistic Regression, которая набрала F1 = 0.689"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and finding new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка текстов включает в себя такие процессы, как токенизация; лемматизация; очистка, которая может избавить от стоп-слов, пунктуации, различных симоволов и сущностей; разметка (морфологическая, синтаксическая и пр.); первод символов в нижний регистр и т.д.\n",
    "\n",
    "Наиболее быстрым способом предобработать тексты позволяют регулярные выражения. Обычно я их использую, чтобы быстро проверить гипотезы о важности токенов для модели. Я определил функцию utils.regexp_tokenizer(texts, lemmatizer, stop_words), в которой основной шаблон для поиска токенов:\n",
    "```\n",
    "\\w+\\d+\\w+?|\\d+\\w+|(?:\\w\\s)+|(?:\\w\\s?\\.\\s?)+\\w?|\\w+\\.+\\w+|\\w+'\\w+|#?\\w+-?\\w+|\\w+[\\*\\$]+\\w+?\n",
    "```\n",
    "\n",
    "Также функция принимает лемматизатор, я использовал WordNetLemmatizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                         | 3239/157571 [00:03<11:50, 217.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No tokens for text:\n",
      " 1993\n",
      "\n",
      "1994\n",
      "\n",
      "1995\n",
      "\n",
      "1996\n",
      "\n",
      "1997\n",
      "\n",
      "1998\n",
      "\n",
      "1999\n",
      "\n",
      "2000\n",
      "\n",
      "2001\n",
      "\n",
      "2002\n",
      "\n",
      "2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                       | 4966/157571 [00:04<01:43, 1468.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No tokens for text:\n",
      " 193.61.111.53  15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▏                                                                 | 15613/157571 [00:10<01:17, 1828.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No tokens for text:\n",
      " ~ \n",
      "\n",
      "68.193.147.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████▌                                                 | 50769/157571 [00:28<00:56, 1879.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No tokens for text:\n",
      " 14:53,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████                                                 | 51926/157571 [00:29<00:57, 1845.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No tokens for text:\n",
      " 92.24.199.233|92.24.199.233]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████▊                                             | 59911/157571 [00:33<00:54, 1807.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No tokens for text:\n",
      " \"\n",
      "\n",
      " 199.209.144.211  \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████▍                                   | 80860/157571 [00:44<00:39, 1930.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No tokens for text:\n",
      " \"\n",
      " '''''' 2010/2013 \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████▉| 157564/157571 [01:23<00:00, 1876.39it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_texts = regexp_tokenizer(data.comment_text, lemmatizer, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Barcelona football seasons (early 20th century) \\n\\nI looked at the Barcelona football seasons for which you added  (and some other seasons after). They could do with , really. (I don't think it's worth listing at WP:PNT, but this should put them in view of  one way or another, or if not that could be added to their talk pages). I don't want to do it myself (a) because I am lazy and (b) because I know nothing about football and would probably make translation mistakes.\",\n",
       "       '\"\\n\\n answering questions \\n\\nI won\\'t have a chance to say hello or goodbye when I\\'m blocked/banned/whatever. So, \"\"Hello; I\\'m not holding my breath on anyone addressing your questions concerning Dawn Well\\'s page on Wiki; and I better get in a good-bye, as I feel I have the lifespan of a May fly in a low ceilinged room full of bullfrogs; not because I’m wrong, but because anyone on your side of the issue is going to be attacked and baited into some infraction, an infraction ignored by someone they like, but an infraction that will be worthy of execution when committed by me.”\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.comment_text[:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barcelona football season early th century looked barcelona football season added season could really think worth listing wp pnt put view one way another could added talk page want lazy know nothing football would probably make translation mistake',\n",
       " \"answering question chance say hello goodbye i'm blocked banned whatever hello i'm holding breath anyone addressing question concerning dawn well's page wiki better get good-bye feel lifespan may fly low ceilinged room full bullfrog wrong anyone side issue going attacked baited infraction infraction ignored someone like infraction worthy execution committed\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, для некоторых текстов не нашлось токенов. Это из-за того, что я игнорировал цифры, знаки пунктуации, а сосредоточился только на словах. Стоит отметить, что для достижения наилучших результатов на нашем датасете, возможно, стоит использовать ip адреса, даты и прочее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе исследования стало понятно, что spacy со своим лемматизатором без очистки, кроме стоп-слов, показывает лучшие результаты, нежели мой regexp_tokenizer() и сама spacy с очисткой от пунктуации и прочих символов. Но у таких инструментов, как spacy есть существенный минус, они требуют больше времени, чем регулярные выражения. Предобработка с помощью spacy заняла у меня более 50 минут. Кроме того, spacy позволяет получить части речи, что также стоит использовать как дополнительный признак для текстов. Интуиция здесь следующая - в токсичных комментариях грамматические формы отличаются от форм, которые используются в обычных комментариях. Я не нашел, как получить тэги для всех морфем словоформы в spacy (в mystem такое есть), поэтому использовал только части речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_lemmatizer возвращает (lemmatized_texts, tags) \n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "#data['lemmatized_text'], data['tags'] = spacy_lemmatizer(data.comment_text, nlp)\n",
    "\n",
    "\n",
    "# я сохранил у себя на диске предобработанный датасет, поэтому буду пользоваться им \n",
    "data = pd.read_csv(data_folder + '/train_spacy3_tagged.csv')\n",
    "data = data[['comment_text', 'lemmatized_text', 'tags', 'toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>tags</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barcelona football seasons (early 20th century...</td>\n",
       "      <td>barcelona football season ( early 20th century...</td>\n",
       "      <td>NNP NN NNS -LRB- JJ JJ NN -RRB- _SP PRP VBD IN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\n answering questions \\n\\nI won't have a c...</td>\n",
       "      <td>\" \\n\\n  answer question \\n\\n -pron- will not h...</td>\n",
       "      <td>`` _SP VBG NNS _SP PRP MD RB VB DT NN TO VB UH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\nIssues\\nOnefortyone, the reason why I rem...</td>\n",
       "      <td>\" \\n\\n issue \\n onefortyone , the reason why -...</td>\n",
       "      <td>`` _SP NNS _SP NNP , DT NN WRB PRP VBD PRP$ NN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nIn that case would \"\"new age\"\" at least be ...</td>\n",
       "      <td>\" \\n in that case would \" \" new age \" \" at lea...</td>\n",
       "      <td>`` _SP IN DT NN MD `` `` JJ NN '' `` IN JJS VB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Wolf,\\nWe need to have a serious talk abou...</td>\n",
       "      <td>mr. wolf , \\n -pron- need to have a serious ta...</td>\n",
       "      <td>NNP NNP , _SP PRP VBP TO VB DT JJ NN IN NNP . ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Please note that DPeterson has now starting a...</td>\n",
       "      <td>\" please note that dpeterson have now start ac...</td>\n",
       "      <td>`` UH VB IN NNP VBZ RB VBG VBG PRP IN NN IN VB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bald Eagle\\nI've addressed all your points so ...</td>\n",
       "      <td>bald eagle \\n -pron- have address all -pron- p...</td>\n",
       "      <td>NNP NNP _SP PRP VB VBN DT PRP$ NNS RB RB . NNS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SARAH \\n\\nYou deleted it, but it remains. I em...</td>\n",
       "      <td>sarah \\n\\n -pron- delete -pron- , but -pron- r...</td>\n",
       "      <td>NNP _SP PRP VBD PRP , CC PRP VBZ . PRP VBD NN ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rand Paul \\n\\nAre you monitoring Rand Paul?  I...</td>\n",
       "      <td>rand paul \\n\\n be -pron- monitor rand paul ?  ...</td>\n",
       "      <td>NNP NNP _SP VBP PRP VBG NNP NNP . _SP IN RB , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Poor AFD rationales \\n Wikipedia:Articles for ...</td>\n",
       "      <td>poor afd rationale \\n  wikipedia : article for...</td>\n",
       "      <td>JJ NNP VBZ _SP NNP : NNS IN NN SYM NNP NNP NNP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Barcelona football seasons (early 20th century...   \n",
       "1  \"\\n\\n answering questions \\n\\nI won't have a c...   \n",
       "2  \"\\n\\nIssues\\nOnefortyone, the reason why I rem...   \n",
       "3  \"\\nIn that case would \"\"new age\"\" at least be ...   \n",
       "4  Mr. Wolf,\\nWe need to have a serious talk abou...   \n",
       "5  \"Please note that DPeterson has now starting a...   \n",
       "6  Bald Eagle\\nI've addressed all your points so ...   \n",
       "7  SARAH \\n\\nYou deleted it, but it remains. I em...   \n",
       "8  Rand Paul \\n\\nAre you monitoring Rand Paul?  I...   \n",
       "9  Poor AFD rationales \\n Wikipedia:Articles for ...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  barcelona football season ( early 20th century...   \n",
       "1  \" \\n\\n  answer question \\n\\n -pron- will not h...   \n",
       "2  \" \\n\\n issue \\n onefortyone , the reason why -...   \n",
       "3  \" \\n in that case would \" \" new age \" \" at lea...   \n",
       "4  mr. wolf , \\n -pron- need to have a serious ta...   \n",
       "5  \" please note that dpeterson have now start ac...   \n",
       "6  bald eagle \\n -pron- have address all -pron- p...   \n",
       "7  sarah \\n\\n -pron- delete -pron- , but -pron- r...   \n",
       "8  rand paul \\n\\n be -pron- monitor rand paul ?  ...   \n",
       "9  poor afd rationale \\n  wikipedia : article for...   \n",
       "\n",
       "                                                tags  toxic  \n",
       "0  NNP NN NNS -LRB- JJ JJ NN -RRB- _SP PRP VBD IN...      0  \n",
       "1  `` _SP VBG NNS _SP PRP MD RB VB DT NN TO VB UH...      0  \n",
       "2  `` _SP NNS _SP NNP , DT NN WRB PRP VBD PRP$ NN...      0  \n",
       "3  `` _SP IN DT NN MD `` `` JJ NN '' `` IN JJS VB...      0  \n",
       "4  NNP NNP , _SP PRP VBP TO VB DT JJ NN IN NNP . ...      0  \n",
       "5  `` UH VB IN NNP VBZ RB VBG VBG PRP IN NN IN VB...      0  \n",
       "6  NNP NNP _SP PRP VB VBN DT PRP$ NNS RB RB . NNS...      0  \n",
       "7  NNP _SP PRP VBD PRP , CC PRP VBZ . PRP VBD NN ...      0  \n",
       "8  NNP NNP _SP VBP PRP VBG NNP NNP . _SP IN RB , ...      0  \n",
       "9  JJ NNP VBZ _SP NNP : NNS IN NN SYM NNP NNP NNP...      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  13442636\n",
      "Rare tokens:  122115\n"
     ]
    }
   ],
   "source": [
    "# Посчитаем количество токенов и количество редких токенов\n",
    "all_lemmatized_tokens = [w for t in data.lemmatized_text for w in t.split(' ')]\n",
    "freq = nltk.probability.FreqDist(all_lemmatized_tokens)\n",
    "uncommon = {word for (word, count) in freq.most_common() if count == 1}\n",
    "print('Tokens: ', len(all_lemmatized_tokens))\n",
    "print('Rare tokens: ', len(uncommon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас большое количество токенов, которые встречаются один раз. Здесь можно было бы применить spell correction с помощью TextBlob или других библиотек, но они не справляются со словами, вроде \"hiiiii\", \"heeeeeeellooooo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь подумаем, какие количественные признаки еще мы можем извлечь из текстов.\n",
    "1. Количество ненормативной лексики относительно всех слов в тексте. Это совершенно очевидно, что подобный признак будет полезен.\n",
    "2. Количество заглавных букв относительно всех символов в тексте. Зачастую можно встретить токсичные комментарии, состоящие только из заглавных букв.\n",
    "3. Количество специальных симовлов (эмодзи, разлиные редкие символы, символы, которыми можно что-то изобразить, и пр.). В датасете часто встречаются токсичные комментарии, которые в большей степени состоят из символов. Например: \"\"\"=P=\\n=E=\\n=N=\\n=I=\\n ... \"\"\"\n",
    "4. Количество уникальных слов, относительно всех слов текста. Совершенно понятно, что активная лексика в токсичных комментариях не такая обширная.\n",
    "5. Количество восклицательных знаков.\n",
    "6. Количество \"добрых\" эмодзи, смайликов.\n",
    "7. Длина текста.\n",
    "\n",
    "Я буду использовать 1-4. Для этого я подготовил функции в utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_list = get_list_of_special_characters(data.comment_text)\n",
    "\n",
    "data['nbadwords'] = get_nbadwords(data.lemmatized_text, bad_words)   # 1\n",
    "data['ncapitals'] = get_ncapitals(data.comment_text)                 # 2\n",
    "data['nspecial'] = get_nspecial(data.comment_text, character_list)   # 3\n",
    "data['nuniquewords'] = get_nuniquewords(data.lemmatized_text)        # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>tags</th>\n",
       "      <th>toxic</th>\n",
       "      <th>nbadwords</th>\n",
       "      <th>ncapitals</th>\n",
       "      <th>nspecial</th>\n",
       "      <th>nuniquewords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barcelona football seasons (early 20th century...</td>\n",
       "      <td>barcelona football season ( early 20th century...</td>\n",
       "      <td>NNP NN NNS -LRB- JJ JJ NN -RRB- _SP PRP VBD IN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.027601</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>0.576577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\n answering questions \\n\\nI won't have a c...</td>\n",
       "      <td>\" \\n\\n  answer question \\n\\n -pron- will not h...</td>\n",
       "      <td>`` _SP VBG NNS _SP PRP MD RB VB DT NN TO VB UH...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.022453</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>0.580153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\nIssues\\nOnefortyone, the reason why I rem...</td>\n",
       "      <td>\" \\n\\n issue \\n onefortyone , the reason why -...</td>\n",
       "      <td>`` _SP NNS _SP NNP , DT NN WRB PRP VBD PRP$ NN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.547771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nIn that case would \"\"new age\"\" at least be ...</td>\n",
       "      <td>\" \\n in that case would \" \" new age \" \" at lea...</td>\n",
       "      <td>`` _SP IN DT NN MD `` `` JJ NN '' `` IN JJS VB...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Wolf,\\nWe need to have a serious talk abou...</td>\n",
       "      <td>mr. wolf , \\n -pron- need to have a serious ta...</td>\n",
       "      <td>NNP NNP , _SP PRP VBP TO VB DT JJ NN IN NNP . ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.078608</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.467836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Please note that DPeterson has now starting a...</td>\n",
       "      <td>\" please note that dpeterson have now start ac...</td>\n",
       "      <td>`` UH VB IN NNP VBZ RB VBG VBG PRP IN NN IN VB...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bald Eagle\\nI've addressed all your points so ...</td>\n",
       "      <td>bald eagle \\n -pron- have address all -pron- p...</td>\n",
       "      <td>NNP NNP _SP PRP VB VBN DT PRP$ NNS RB RB . NNS...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SARAH \\n\\nYou deleted it, but it remains. I em...</td>\n",
       "      <td>sarah \\n\\n -pron- delete -pron- , but -pron- r...</td>\n",
       "      <td>NNP _SP PRP VBD PRP , CC PRP VBZ . PRP VBD NN ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rand Paul \\n\\nAre you monitoring Rand Paul?  I...</td>\n",
       "      <td>rand paul \\n\\n be -pron- monitor rand paul ?  ...</td>\n",
       "      <td>NNP NNP _SP VBP PRP VBG NNP NNP . _SP IN RB , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.677419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Poor AFD rationales \\n Wikipedia:Articles for ...</td>\n",
       "      <td>poor afd rationale \\n  wikipedia : article for...</td>\n",
       "      <td>JJ NNP VBZ _SP NNP : NNS IN NN SYM NNP NNP NNP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.111940</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Barcelona football seasons (early 20th century...   \n",
       "1  \"\\n\\n answering questions \\n\\nI won't have a c...   \n",
       "2  \"\\n\\nIssues\\nOnefortyone, the reason why I rem...   \n",
       "3  \"\\nIn that case would \"\"new age\"\" at least be ...   \n",
       "4  Mr. Wolf,\\nWe need to have a serious talk abou...   \n",
       "5  \"Please note that DPeterson has now starting a...   \n",
       "6  Bald Eagle\\nI've addressed all your points so ...   \n",
       "7  SARAH \\n\\nYou deleted it, but it remains. I em...   \n",
       "8  Rand Paul \\n\\nAre you monitoring Rand Paul?  I...   \n",
       "9  Poor AFD rationales \\n Wikipedia:Articles for ...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  barcelona football season ( early 20th century...   \n",
       "1  \" \\n\\n  answer question \\n\\n -pron- will not h...   \n",
       "2  \" \\n\\n issue \\n onefortyone , the reason why -...   \n",
       "3  \" \\n in that case would \" \" new age \" \" at lea...   \n",
       "4  mr. wolf , \\n -pron- need to have a serious ta...   \n",
       "5  \" please note that dpeterson have now start ac...   \n",
       "6  bald eagle \\n -pron- have address all -pron- p...   \n",
       "7  sarah \\n\\n -pron- delete -pron- , but -pron- r...   \n",
       "8  rand paul \\n\\n be -pron- monitor rand paul ?  ...   \n",
       "9  poor afd rationale \\n  wikipedia : article for...   \n",
       "\n",
       "                                                tags  toxic  nbadwords  \\\n",
       "0  NNP NN NNS -LRB- JJ JJ NN -RRB- _SP PRP VBD IN...      0   0.009009   \n",
       "1  `` _SP VBG NNS _SP PRP MD RB VB DT NN TO VB UH...      0   0.007634   \n",
       "2  `` _SP NNS _SP NNP , DT NN WRB PRP VBD PRP$ NN...      0   0.006369   \n",
       "3  `` _SP IN DT NN MD `` `` JJ NN '' `` IN JJS VB...      0   0.000000   \n",
       "4  NNP NNP , _SP PRP VBP TO VB DT JJ NN IN NNP . ...      0   0.005848   \n",
       "5  `` UH VB IN NNP VBZ RB VBG VBG PRP IN NN IN VB...      0   0.011494   \n",
       "6  NNP NNP _SP PRP VB VBN DT PRP$ NNS RB RB . NNS...      0   0.000000   \n",
       "7  NNP _SP PRP VBD PRP , CC PRP VBZ . PRP VBD NN ...      0   0.000000   \n",
       "8  NNP NNP _SP VBP PRP VBG NNP NNP . _SP IN RB , ...      0   0.032258   \n",
       "9  JJ NNP VBZ _SP NNP : NNS IN NN SYM NNP NNP NNP...      0   0.035714   \n",
       "\n",
       "   ncapitals  nspecial  nuniquewords  \n",
       "0   0.027601  0.010616      0.576577  \n",
       "1   0.022453  0.006908      0.580153  \n",
       "2   0.029891  0.001359      0.547771  \n",
       "3   0.015152  0.015152      0.750000  \n",
       "4   0.078608  0.002577      0.467836  \n",
       "5   0.026247  0.000000      0.597701  \n",
       "6   0.079365  0.000000      0.875000  \n",
       "7   0.056604  0.000000      0.687500  \n",
       "8   0.062500  0.008929      0.677419  \n",
       "9   0.111940  0.014925      0.678571  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посморим как коррелируют новые признаки с классами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Построим тепловую карту для матрицы корреляций признаков и целевого значения\n",
    "corr = data[['nspecial', 'nuniquewords', 'ncapitals', 'nbadwords', 'toxic']].corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целевой признак хорошо коррелирует с ненормативной лексикой и заглавными буквами. Остальные признаки коррелируют незначительно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала извлекаем новые признаки и объединяем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',                                 \n",
    "    tokenizer=split_tokenizer,                       # Все токены разделены пробелом, поэтому токенизируем разделением по ' '\n",
    "    stop_words='english',                             \n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000,                              \n",
    "    max_df=0.5)                                      # и мы не учитываем токены, которые встречаются более чем в 50 процентов текстов.\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(                   # Также используем символьные n-грамы из исходных текстов в качестве признаков\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=20000,\n",
    "    max_df=0.5)\n",
    "\n",
    "tag_vectorizer = TfidfVectorizer(                   # Извлечем признаки из частей речи в текстах.\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000,\n",
    "    max_df=0.5)\n",
    "\n",
    "train_w_features = word_vectorizer.fit_transform(data.lemmatized_text)\n",
    "train_c_features = char_vectorizer.fit_transform(data.comment_text)\n",
    "train_t_features = tag_vectorizer.fit_transform(data.tags)\n",
    "\n",
    "for w in bad_words:\n",
    "    if w in word_vectorizer.vocabulary_:\n",
    "        ind = word_vectorizer.vocabulary_[w]\n",
    "        train_w_features[:, ind] *= 5.0\n",
    "    if w in char_vectorizer.vocabulary_:\n",
    "        ind = char_vectorizer.vocabulary_[w]\n",
    "        train_c_features[:, ind] *= 5.0\n",
    "\n",
    "\n",
    "train_features = hstack([train_w_features, train_c_features, train_t_features, \n",
    "                         data.nbadwords.values.reshape(-1, 1), data.nuniquewords.values.reshape(-1, 1), \n",
    "                         data.nspecial.values.reshape(-1, 1), data.ncapitals.values.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Увеличим значения TF-IDF для ненормативной лексики\n",
    "\n",
    "alpha = 5.0\n",
    "\n",
    "for w in bad_words:\n",
    "    if w in word_vectorizer.vocabulary_:\n",
    "        ind = word_vectorizer.vocabulary_[w]\n",
    "        train_w_features[:, ind] *= alpha\n",
    "    if w in char_vectorizer.vocabulary_:\n",
    "        ind = char_vectorizer.vocabulary_[w]\n",
    "        train_c_features[:, ind] *= alpha\n",
    "\n",
    "\n",
    "train_features = hstack([train_w_features, train_c_features, train_t_features, \n",
    "                         data.nbadwords.values.reshape(-1, 1), data.nuniquewords.values.reshape(-1, 1), \n",
    "                         data.nspecial.values.reshape(-1, 1), data.ncapitals.values.reshape(-1,1)])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores for non-toxic: \n",
      " [('', 369835.54010625527), ('\"', 14903.215870510438), ('\\n\\n', 5361.5996355190355), ('god', 4783.520303355421), ('\\n', 4418.009772879224), ('?', 4384.392314043034), (')', 4033.7399747456466), ('article', 3938.4806403005623), ('(', 3757.3467954638286), (':', 3751.523026009319), ('-', 3715.2235314174554), ('page', 3534.207398680681), ('!', 3133.8678187249798), ('kill', 3084.1105321054533), ('talk', 3006.6285972064534)]\n",
      "TF-IDF scores for toxic: \n",
      " [('fuck', 21268.754097442554), ('', 20920.57338504264), ('shit', 7856.964349522259), ('fucking', 7544.92680046646), ('suck', 6957.662831486181), ('bitch', 5621.759681927644), ('ass', 5605.0476591584265), ('stupid', 5557.197139726444), ('asshole', 4765.726561547852), ('dick', 4121.229522515433), ('gay', 4089.893262602097), ('faggot', 4010.221173924781), ('cunt', 3875.2424676853657), ('hell', 3509.072986856047), ('cock', 2486.2879555972972)]\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на топ признаков для разных классов.\n",
    "top_n = 15\n",
    "\n",
    "print('TF-IDF scores for non-toxic: \\n', sorted(list(zip(word_vectorizer.get_feature_names(), \n",
    "                                             train_w_features[data.toxic.values == 0].sum(0).getA1())), \n",
    "                                 key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "print('TF-IDF scores for toxic: \\n', sorted(list(zip(word_vectorizer.get_feature_names(), \n",
    "                                             train_w_features[data.toxic.values == 1].sum(0).getA1())), \n",
    "                                 key=lambda x: x[1], reverse=True)[:top_n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все кажется вполне адекватным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score is 0.781008259781207\n"
     ]
    }
   ],
   "source": [
    "# Тренируем модель\n",
    "model = LogisticRegression(solver='sag')\n",
    "cv_score = np.mean(cross_val_score(model, train_features, data.toxic, cv=3, scoring='f1'))\n",
    "print('CV score is {}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6670440\ttest: 0.6862795\tbest: 0.6862795 (0)\ttotal: 19.8ms\tremaining: 39.6s\n",
      "100:\tlearn: 0.7361347\ttest: 0.7426901\tbest: 0.7429963 (98)\ttotal: 1.89s\tremaining: 35.6s\n",
      "200:\tlearn: 0.7566251\ttest: 0.7606003\tbest: 0.7606909 (199)\ttotal: 3.63s\tremaining: 32.5s\n",
      "300:\tlearn: 0.7695121\ttest: 0.7673566\tbest: 0.7673566 (300)\ttotal: 5.35s\tremaining: 30.2s\n",
      "400:\tlearn: 0.7770147\ttest: 0.7714623\tbest: 0.7719340 (396)\ttotal: 7.07s\tremaining: 28.2s\n",
      "500:\tlearn: 0.7828782\ttest: 0.7733741\tbest: 0.7737913 (494)\ttotal: 8.77s\tremaining: 26.2s\n",
      "600:\tlearn: 0.7864395\ttest: 0.7739509\tbest: 0.7742619 (592)\ttotal: 10.5s\tremaining: 24.4s\n",
      "700:\tlearn: 0.7927057\ttest: 0.7758742\tbest: 0.7761474 (691)\ttotal: 12.2s\tremaining: 22.6s\n",
      "800:\tlearn: 0.7984901\ttest: 0.7777518\tbest: 0.7777648 (775)\ttotal: 13.9s\tremaining: 20.8s\n",
      "900:\tlearn: 0.8016006\ttest: 0.7784151\tbest: 0.7784151 (900)\ttotal: 15.6s\tremaining: 19.1s\n",
      "1000:\tlearn: 0.8050955\ttest: 0.7786866\tbest: 0.7792724 (970)\ttotal: 17.3s\tremaining: 17.3s\n",
      "1100:\tlearn: 0.8086509\ttest: 0.7792572\tbest: 0.7798272 (1090)\ttotal: 19.1s\tremaining: 15.6s\n",
      "1200:\tlearn: 0.8125759\ttest: 0.7795937\tbest: 0.7798272 (1090)\ttotal: 20.8s\tremaining: 13.8s\n",
      "1300:\tlearn: 0.8150499\ttest: 0.7803853\tbest: 0.7806700 (1296)\ttotal: 22.5s\tremaining: 12.1s\n",
      "1400:\tlearn: 0.8166772\ttest: 0.7804366\tbest: 0.7807611 (1321)\ttotal: 24.3s\tremaining: 10.4s\n",
      "1500:\tlearn: 0.8203858\ttest: 0.7808235\tbest: 0.7811990 (1464)\ttotal: 26s\tremaining: 8.64s\n",
      "1600:\tlearn: 0.8227855\ttest: 0.7808347\tbest: 0.7812391 (1502)\ttotal: 27.8s\tremaining: 6.92s\n",
      "1700:\tlearn: 0.8254351\ttest: 0.7808459\tbest: 0.7814940 (1615)\ttotal: 29.5s\tremaining: 5.18s\n",
      "1800:\tlearn: 0.8285796\ttest: 0.7810279\tbest: 0.7814940 (1615)\ttotal: 31.2s\tremaining: 3.45s\n",
      "1900:\tlearn: 0.8315572\ttest: 0.7804196\tbest: 0.7817271 (1809)\ttotal: 32.9s\tremaining: 1.72s\n",
      "1999:\tlearn: 0.8337151\ttest: 0.7818585\tbest: 0.7822844 (1970)\ttotal: 34.7s\tremaining: 0us\n",
      "bestTest = 0.7822843823\n",
      "bestIteration = 1970\n",
      "Shrink model to first 1971 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на результаты catboost\n",
    "def fit_catboost(X_train, X_test, y_train, y_test, catboost_params={}, verbose=100):\n",
    "    learn_pool = Pool(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        text_features=['comment_text', 'lemmatized_text', 'tags'],\n",
    "        feature_names=list(X_train)\n",
    "    )\n",
    "    test_pool = Pool(\n",
    "        X_test, \n",
    "        y_test, \n",
    "        text_features=['comment_text', 'lemmatized_text', 'tags'],\n",
    "        feature_names=list(X_train)\n",
    "    )\n",
    "    \n",
    "    catboost_default_params = {\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': 0.05,\n",
    "        'eval_metric': 'F1',\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    \n",
    "    catboost_default_params.update(catboost_params)\n",
    "    \n",
    "    model = CatBoostClassifier(**catboost_default_params)\n",
    "    model.fit(learn_pool, eval_set=test_pool, verbose=verbose)\n",
    "\n",
    "    return model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[[column for column in data.columns if column != 'toxic']],\n",
    "                                                    data.toxic.values, test_size=0.33, random_state=42)\n",
    "cat_boost_model = fit_catboost(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вполне возможно n-грамы помогут еше улучшить модель. \n",
    "\n",
    "Ну, а теперь, чтобы порадовать глаз, можно использовать ROC-AUC =) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score is 0.9773713992871795\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='sag')\n",
    "cv_score = np.mean(cross_val_score(model, train_features, data.toxic, cv=3, scoring='roc_auc'))\n",
    "print('CV score is {}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно посмотреть тексты, где модель ошибается, и построить гипотезы о возможных улучшаениях на стадии предобработки. Далее мы можем попробовать более сложные модели, сделать grid search для подбора лучших параметров моделей. Например, стоит попробовать word embeddings и подавать их на вход LSTM или CNN, а лосс функцию подогнать под целевую метрику. Для достижения лучшего результата по метрикам нужно натренировать много разных моделей, например, Log Reg, Random Forest (к тому же эту модель можно использовать для определения важности признаков), Bagging, Catboost, LSTM, CNN, а затем сделать из них ансамбль. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNekIZpu5IMEGZwFfLv0cgd",
   "collapsed_sections": [
    "QSpJvCF4C3mo",
    "OiwVeaV_lQYJ"
   ],
   "machine_shape": "hm",
   "name": "Toxic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
